\blue{
First person point of view video is a video produced by 
egocentic camera that capture the scene with first person perspective. 
The characteristic of the first-person point of view video 
in general features are rich in noise and contain plenty of uncontrolled variations scene.
Moreover, these videos need to deal with some adverse effects such as shaky frame, background clutter, occlusion and inter-class variation. Coupled with the absence of the actor's pose, 
it makes the first-person video is more challenging than the third person video.
}

This \blue{thesis} presents a new approach for action recognition in
first-person videos which aggregates both of the short- and
long-term trends based on the coefficients of the Hilbert-Huang
transform (HHT), a renowned time-frequency analysis tool. In
contrast to previous works like Pooled Time Series (PoT), the new
approach can extract the salient features of activities based on the
non-stationary HHT analysis, which consists of empirical mode
decomposition and Hilbert spectral analysis. The proposed approach
can be incorporated with the convolutional neural network (CNN)
features such as trajectory pooled CNN features to
achieve superior detection accuracy. Simulations show that
the proposed method outperforms the main state-of-the-art works on
\blue{three} widespread public first-person datasets.

\textbf{Keywords}: temporal pyramid pooling, temporal
aggregation, first-person video, action recognition, Hilbert-Huang transform